Next up:
- Check dataset batching
- Drop remainder?
- Re-try testing (with toy tfrec)

Longer term:
- Change package name
- Viz helpers for training
- Rerun baseline models
    - Non fine-tuned BERT
    - Simple classifier
    - FFN with increasing complexity

Other:
- Rerun fine-tuning
    - Margin
    - Grad accumulation
    - Loss function
    - Model type
- Any manipulation needed for the optimizer?
- Should loss consider distance from average of anchor encodings, or average distance from each anchor encoding?
- Add maximize distance between negative and anchor to loss?
- Should we consider limiting string length