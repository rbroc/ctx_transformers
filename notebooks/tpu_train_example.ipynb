{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd0ba954e04f94d5e4970321c570fb05eff7c202d9f1ab5b8ba406812668fc94516",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Model training\n",
    "This notebook outlines all the steps needed to train models from this library. <br>\n",
    "ATM, it does not assume distributed training, but that requires very small adaptations (mostly TF one-liners), i.e.,:\n",
    "- Initializing the TPU (not needed if training on GPU)\n",
    "- Defining the training Strategy;\n",
    "- Distributing the dataset over the replicas;\n",
    "- Passing the strategy to the Trainer, setting distributed=True, and passing device for checkpointing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Imports and mount drive"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reddit.utils import (load_tfrecord, pad_and_stack,\n",
    "                          split_dataset)\n",
    "from reddit.models import BatchTransformer\n",
    "from reddit.losses import TripletLossBase\n",
    "from reddit.training import Trainer\n",
    "from transformers import TFDistilBertModel\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_PATH = Path('..') / 'logs' / 'sample_output'\n",
    "METRICS_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "source": [
    "### Dataset\n",
    "Load dataset, pad to desired length, batch and distribute"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_params = {'n_anchor': 20,\n",
    "             'n_pos': 1,\n",
    "             'n_neg': 1,\n",
    "             'batch_size': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = glob.glob('../reddit/data/datasets/triplet/*')\n",
    "ds = load_tfrecord(fs)\n",
    "ds = pad_and_stack(ds, pad_to=[ds_params['n_anchor'], \n",
    "                               ds_params['n_pos'], \n",
    "                               ds_params['n_neg']]).batch(ds_params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_val, ds_test = split_dataset(ds, size=1000,\n",
    "                                          perc_train=.1, \n",
    "                                          perc_val=.01,\n",
    "                                          perc_test=.02)"
   ]
  },
  {
   "source": [
    "### Initialize training parametes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'weights': 'distilbert-base-uncased',\n",
    "                'model': TFDistilBertModel,\n",
    "                'optimizer_learning_rate': 2e-5,\n",
    "                'optimizer_n_train_steps': 100,\n",
    "                'optimizer_n_warmup_steps': 10,\n",
    "                'loss_margin': 1,\n",
    "                'n_epochs': 10,\n",
    "                'steps_per_epoch': 100,\n",
    "                'train_vars': ['losses','metrics', \n",
    "                               'dist_pos', 'dist_neg', \n",
    "                               'dist_anchor'],\n",
    "                'test_vars': ['test_losses', 'test_metrics',\n",
    "                              'test_dist_pos', 'test_dist_neg',\n",
    "                              'test_dist_anchor'],\n",
    "                'log_every': 50}"
   ]
  },
  {
   "source": [
    "### Initialize optimizer, model, loss, and trainer object"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "optimizer = create_optimizer(train_params['optimizer_learning_rate'],\n",
    "                             num_train_steps=train_params['optimizer_n_train_steps'], \n",
    "                             num_warmup_steps=train_params['optimizer_n_warmup_steps'])\n",
    "model = BatchTransformer(train_params['model'], \n",
    "                         train_params['weights'])\n",
    "loss = TripletLossBase(train_params['loss_margin'],\n",
    "                       n_pos=ds_params['n_pos'],\n",
    "                       n_neg=ds_params['n_neg'])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model,\n",
    "                  loss,\n",
    "                  optimizer,\n",
    "                  strategy=None, \n",
    "                  n_epochs=train_params['n_epoch'], \n",
    "                  steps_per_epoch=train_params['steps_per_epoch'], \n",
    "                  log_every=train_params['log_every'],\n",
    "                  train_vars=train_params['train_vars'], \n",
    "                  test_vars=train_params['test_vars'], \n",
    "                  log_path=str(METRICS_PATH),\n",
    "                  checkpoint_device=None,\n",
    "                  distributed=False)"
   ]
  },
  {
   "source": [
    "### Train!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(dataset_train=ds_train, \n",
    "              dataset_test=ds_val)"
   ]
  }
 ]
}