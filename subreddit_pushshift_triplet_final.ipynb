{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from tensorflow.keras import layers, Input\n",
    "from transformers import TFDistilBertModel\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from tools.datasets import load_tfrecord_triplet_nn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to add baseline model\n",
    "# Define validation loop\n",
    "# Check log metrics\n",
    "# Check if loss significantly reduces speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')]"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "encoder_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(keras.losses.Loss):\n",
    "\n",
    "    ''' Compute triplet loss for Bert encodings'''\n",
    "\n",
    "    def __init__(self, margin=0, raw_enc=True):\n",
    "        super().__init__()\n",
    "        self.margin = tf.constant(margin, dtype=tf.float32)\n",
    "        self.raw_enc = raw_enc\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        if self.raw_enc:\n",
    "            a_emb = keras.backend.mean(y_pred.last_hidden_state[:, :-2, :], axis=1)\n",
    "            p_emb = y_pred.last_hidden_state[:, -2, :]\n",
    "            n_emb = y_pred.last_hidden_state[:, -1, :]\n",
    "        else:\n",
    "            a_emb = y_pred[0]\n",
    "            p_emb = y_pred[1]\n",
    "            n_emb = y_pred[2]\n",
    "        print(a_emb.shape)\n",
    "        d_pos = tf.reduce_sum(tf.square(a_emb - p_emb), 1)\n",
    "        d_neg = tf.reduce_sum(tf.square(a_emb - n_emb), 1)\n",
    "        loss_val = tf.maximum(0.0, self.margin + d_pos - d_neg)\n",
    "        return tf.reduce_mean(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletAccuracy(tf.keras.metrics.Metric):\n",
    "      \n",
    "    ''' Metric tracking whether model assigns lower distance to\n",
    "        positive example '''\n",
    "    \n",
    "    def __init__(self, name='accuracy', **kwargs):\n",
    "        super(TripletAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.correct = tf.Variable(0, dtype=tf.float32)\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        corr = 1. if y_pred == 0.0 else 0.\n",
    "        self.correct.assign_add(tf.reduce_mean(corr))\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.correct.assign(0)"
   ]
  },
  {
   "source": [
    "## NNet 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "input_ids = Input(shape=(512), dtype='int32', name='input_ids')\n",
    "encodings = encoder_model(input_ids)\n",
    "model1 = keras.Model(input_ids, encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "tf_distil_bert_model_1 (TFDi TFBaseModelOutput(last_hi 66362880  \n",
      "=================================================================\n",
      "Total params: 66,362,880\n",
      "Trainable params: 66,362,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.trainable = True\n",
    "model1.summary()"
   ]
  },
  {
   "source": [
    "loss_model1 = TripletLoss(margin=.5, raw_enc=True)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=5e-5)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 68,
   "outputs": []
  },
  {
   "source": [
    "## NNet 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_shape = 128\n",
    "n_dense = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNModel(keras.Model):\n",
    "\n",
    "    ''' BERT encodings are passed to feedforward layers \n",
    "        Number and size can vary '''\n",
    "\n",
    "    def __init__(self, encoder, \n",
    "                dense_shape=128, n_dense=3, \n",
    "                trainable=False,\n",
    "                dense_act='relu', name=None):\n",
    "        super(FFNModel, self).__init__(name=name)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.encoder.trainable = trainable\n",
    "        self.dense_layers = keras.Sequential([keras.layers.Dense(dense_shape, \n",
    "                                                                 activation=dense_act) \n",
    "                                            for _ in range(n_dense)])\n",
    "        self.output_signature = tf.float32\n",
    "\n",
    "    def call(self, input):\n",
    "        enc = self.encoder(input)\n",
    "        x_p = self.dense_layers(enc.last_hidden_state[:,-2,:])\n",
    "        x_n = self.dense_layers(enc.last_hidden_state[:,-1,:])\n",
    "        x_a = keras.backend.mean(enc.last_hidden_state[:,:-2,:], axis=1)\n",
    "        x_a = self.dense_layers(x_a)\n",
    "        return x_a, x_p, x_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "ffn_out = FFNModel(encoder_model, dense_shape=dense_shape, n_dense=n_dense)(input_ids)\n",
    "model2 = keras.Model(input_ids, ffn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "ffn_model_2 (FFNModel)       ((None, 128), (None, 128) 66494336  \n",
      "=================================================================\n",
      "Total params: 66,494,336\n",
      "Trainable params: 131,456\n",
      "Non-trainable params: 66,362,880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_model2 = TripletLoss(margin=.5, raw_enc=False)"
   ]
  },
  {
   "source": [
    "## Load the datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames_triplet = glob.glob('datasets/example/triplet_nn1/*')\n",
    "ds_triplet = load_tfrecord_triplet_nn1(filenames=fnames_triplet, \n",
    "                                       compression_type='GZIP')"
   ]
  },
  {
   "source": [
    "## Training protocol"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = TripletAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gradient(opt, model, loss_obj, x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc = model(x)\n",
    "        loss = loss_obj(y_true=None, y_pred=enc)\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    opt.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    return enc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def run_epoch(opt, model, loss_obj, training_data):\n",
    "    losses = []\n",
    "    metrics = []\n",
    "    pbar = tqdm(total=len(list(enumerate(training_data))), leave=False)\n",
    "    for step, x in enumerate(training_data):\n",
    "        enc, loss = apply_gradient(opt, model, loss_obj, x)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        metric.update_state(y_true=None, y_pred=loss)\n",
    "        metric.append(metric.result)\n",
    "\n",
    "        pbar.set_description(f'Step {int(step)}: loss={float(loss)}')\n",
    "        pbar.update()\n",
    "    return losses"
   ]
  },
  {
   "source": [
    "Define general training function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(n_epochs, optimizer, model, loss_fn, training_data):\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f'Start of epoch: {int(epoch)}')\n",
    "        losses_train = run_epoch(optimizer, model, \n",
    "                                 loss_fn, training_data, training)\n",
    "        losses_train_mean = np.mean(losses_train)\n",
    "        metric.reset_states()\n",
    "        print(f'Epoch {int(epoch)}; \\\n",
    "              train loss {losses_train_mean}')\n",
    "    return losses_train, metric"
   ]
  },
  {
   "source": [
    "Train FNN "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start of epoch: 0\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "Step 0: loss=0.49529749155044556:   0%|          | 0/10 [00:14<?, ?it/s]\n",
      "\n",
      "Step 0: loss=0.49529749155044556:  10%|█         | 1/10 [00:14<02:07, 14.17s/it]\n",
      "\n",
      "Step 1: loss=0.6795383095741272:  10%|█         | 1/10 [00:20<02:07, 14.17s/it] \n",
      "\n",
      "Step 1: loss=0.6795383095741272:  20%|██        | 2/10 [00:20<01:34, 11.81s/it]\n",
      "\n",
      "Step 2: loss=1.2755873203277588:  20%|██        | 2/10 [00:33<01:34, 11.81s/it]\n",
      "\n",
      "Step 2: loss=1.2755873203277588:  30%|███       | 3/10 [00:33<01:25, 12.16s/it]\n",
      "\n",
      "Step 3: loss=0.2298920899629593:  30%|███       | 3/10 [00:37<01:25, 12.16s/it]\n",
      "\n",
      "Step 3: loss=0.2298920899629593:  40%|████      | 4/10 [00:37<00:57,  9.62s/it]\n",
      "\n",
      "Step 4: loss=2.9019365310668945:  40%|████      | 4/10 [00:43<00:57,  9.62s/it]\n",
      "\n",
      "Step 4: loss=2.9019365310668945:  50%|█████     | 5/10 [00:43<00:43,  8.61s/it]\n",
      "\n",
      "Step 5: loss=0.1821185052394867:  50%|█████     | 5/10 [00:49<00:43,  8.61s/it]\n",
      "\n",
      "Step 5: loss=0.1821185052394867:  60%|██████    | 6/10 [00:49<00:31,  7.85s/it]\n",
      "\n",
      "Step 6: loss=0.5469990372657776:  60%|██████    | 6/10 [00:54<00:31,  7.85s/it]\n",
      "\n",
      "Step 6: loss=0.5469990372657776:  70%|███████   | 7/10 [00:54<00:20,  6.89s/it]\n",
      "\n",
      "Step 7: loss=0.06347127258777618:  70%|███████   | 7/10 [00:58<00:20,  6.89s/it]\n",
      "\n",
      "Step 7: loss=0.06347127258777618:  80%|████████  | 8/10 [00:58<00:12,  6.02s/it]\n",
      "\n",
      "Step 8: loss=0.35138025879859924:  80%|████████  | 8/10 [01:07<00:12,  6.02s/it]\n",
      "\n",
      "Step 8: loss=0.35138025879859924:  90%|█████████ | 9/10 [01:07<00:06,  7.00s/it]\n",
      "\n",
      "Step 9: loss=0.8338621854782104:  90%|█████████ | 9/10 [01:13<00:06,  7.00s/it] \n",
      "\n",
      "Step 9: loss=0.8338621854782104: 100%|██████████| 10/10 [01:13<00:00,  6.58s/it]Epoch 0;               train loss 0.7560083270072937\n"
     ]
    }
   ],
   "source": [
    "losses_nn1_nt, metric_nn1_nt = run_training(n_epochs=1, \n",
    "                                            optimizer=optimizer, \n",
    "                                            model=model1, \n",
    "                                            loss_fn=loss_model1, \n",
    "                                            training_data=ds_triplet.take(10)) \n",
    "# run with validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_nn1, metric_nn1 = run_training(n_epochs=1, \n",
    "                                      optimizer=optimizer, \n",
    "                                      model=model1, \n",
    "                                      loss_fn=loss_model1, \n",
    "                                      training_data=ds_triplet.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_nn2, metric_nn2 = run_training(n_epochs=1, \n",
    "                                      optimizer=optimizer, \n",
    "                                      model=model2, \n",
    "                                      loss_fn=loss_model2, \n",
    "                                      training_data=ds_triplet.take(10))"
   ]
  }
 ]
}