{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from tensorflow.keras import layers, Input\n",
    "from transformers import TFDistilBertModel\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from tools.datasets import load_tfrecord_triplet_nn1\n",
    "from tensorflow.keras.utils import Progbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTSTANDING ISSUES\n",
    "# Why is model re-initialized in graph?\n",
    "# Several calls to tf.function?\n",
    "# Set up parallelization strategies\n",
    "# Prefetch?\n",
    "# Fix repeated training for model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "encoder_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(keras.losses.Loss):\n",
    "\n",
    "    ''' Compute triplet loss for Bert encodings'''\n",
    "\n",
    "    def __init__(self, margin=0, raw_enc=True):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.raw_enc = raw_enc\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        if self.raw_enc:\n",
    "            a_emb = tf.reduce_mean(y_pred.last_hidden_state[:, :-2, :], axis=1)\n",
    "            p_emb = y_pred.last_hidden_state[:, -2, :]\n",
    "            n_emb = y_pred.last_hidden_state[:, -1, :]\n",
    "        else:\n",
    "            a_emb = y_pred[0]\n",
    "            p_emb = y_pred[1]\n",
    "            n_emb = y_pred[2]\n",
    "        d_pos = tf.reduce_sum(tf.square(a_emb - p_emb), 1)\n",
    "        d_neg = tf.reduce_sum(tf.square(a_emb - n_emb), 1)\n",
    "        loss_val = tf.maximum(0.0, self.margin + d_pos - d_neg)\n",
    "        loss_val = tf.reduce_mean(loss_val)\n",
    "        return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletAccuracy(tf.keras.metrics.Metric):\n",
    "      \n",
    "    ''' Metric tracking whether model assigns lower distance to\n",
    "        positive example '''\n",
    "    \n",
    "    def __init__(self, name='accuracy', margin=.5, **kwargs):\n",
    "        super(TripletAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.correct = tf.Variable(0, dtype=tf.float32)\n",
    "        self.margin = tf.constant(margin, dtype=tf.float32)\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        corr = tf.cast(tf.math.greater(self.margin, y_pred), tf.float32)\n",
    "        self.correct.assign(tf.reduce_mean(corr))\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.correct.assign(0.)"
   ]
  },
  {
   "source": [
    "## NNet 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x10958ae50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x10958ae50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "input_ids = Input(shape=(512), dtype='int32', name='input_ids')\n",
    "encodings = encoder_model(input_ids)\n",
    "model1 = keras.Model(input_ids, encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "tf_distil_bert_model (TFDist TFBaseModelOutput(last_hi 66362880  \n",
      "=================================================================\n",
      "Total params: 66,362,880\n",
      "Trainable params: 66,362,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.trainable = True\n",
    "model1.summary()"
   ]
  },
  {
   "source": [
    "loss_model1 = TripletLoss(margin=.5, raw_enc=True)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=5e-5)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 9,
   "outputs": []
  },
  {
   "source": [
    "## NNet 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_shape = 128\n",
    "n_dense = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNModel(keras.Model):\n",
    "\n",
    "    ''' BERT encodings are passed to feedforward layers \n",
    "        Number and size can vary '''\n",
    "\n",
    "    def __init__(self, encoder, \n",
    "                dense_shape=128, n_dense=3, \n",
    "                trainable=False,\n",
    "                dense_act='relu', name=None):\n",
    "        super(FFNModel, self).__init__(name=name)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.encoder.trainable = trainable\n",
    "        self.dense_layers = keras.Sequential([keras.layers.Dense(dense_shape, \n",
    "                                                                 activation=dense_act) \n",
    "                                            for _ in range(n_dense)])\n",
    "        self.output_signature = tf.float32\n",
    "\n",
    "    def call(self, input):\n",
    "        enc = self.encoder(input)\n",
    "        x_p = self.dense_layers(enc.last_hidden_state[:,-2,:])\n",
    "        x_n = self.dense_layers(enc.last_hidden_state[:,-1,:])\n",
    "        x_a = keras.backend.mean(enc.last_hidden_state[:,:-2,:], axis=1)\n",
    "        x_a = self.dense_layers(x_a)\n",
    "        return x_a, x_p, x_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "ffn_out = FFNModel(encoder_model, dense_shape=dense_shape, n_dense=n_dense)(input_ids)\n",
    "model2 = keras.Model(input_ids, ffn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "ffn_model (FFNModel)         ((None, 128), (None, 128) 66494336  \n",
      "=================================================================\n",
      "Total params: 66,494,336\n",
      "Trainable params: 131,456\n",
      "Non-trainable params: 66,362,880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_model2 = TripletLoss(margin=.5, raw_enc=False)"
   ]
  },
  {
   "source": [
    "## Load and split dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames_triplet = glob.glob('datasets/example/triplet_nn1/*')\n",
    "ds_triplet = load_tfrecord_triplet_nn1(filenames=fnames_triplet, \n",
    "                                       compression_type='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 167320\n",
    "n_train = int(167320 * .7)\n",
    "n_val = int(167320 * .1)\n",
    "n_test = 167320 - (n_train + n_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_triplet = ds_triplet.shuffle(100, seed=0)\n",
    "ds_train = ds_triplet.take(n_train)\n",
    "ds_test = ds_triplet.skip(n_train + n_val)\n",
    "ds_val = ds_triplet.skip(n_train).take(n_val)"
   ]
  },
  {
   "source": [
    "## Training protocol"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = TripletAccuracy(margin=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_step(x, model, loss_obj):\n",
    "    enc = model(x)\n",
    "    loss = loss_obj(y_true=None, y_pred=enc)\n",
    "    return enc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def training_step(opt, model, loss_obj, x, forward_only):\n",
    "    ''' Training step with gradient tape'''\n",
    "    if forward_only:\n",
    "        enc, loss = forward_step(x, model, loss_obj)     \n",
    "    else:\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc, loss = forward_step(x, model, loss_obj)\n",
    "            gradients = tape.gradient(loss, model.trainable_weights)\n",
    "            opt.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    return enc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(opt, model, loss_obj, metric_obj, \n",
    "              training_data, n_examples, forward_only):\n",
    "    ''' Run single training epoch and return list of losses and metrics'''\n",
    "    losses, metrics = [], []\n",
    "    pb_i = Progbar(n_examples, stateful_metrics=['loss', 'correct'])\n",
    "\n",
    "    for x in training_data:\n",
    "        enc, loss = training_step(opt, model, loss_obj, x, forward_only)\n",
    "        losses.append(loss)\n",
    "\n",
    "        metric_obj.update_state(y_true=None, y_pred=loss)\n",
    "        metrics.append(metric_obj.result())\n",
    "        \n",
    "        pb_i.add(1, values=[('loss', loss), ('correct', int(metric_obj.result()))])\n",
    "    return losses, metrics"
   ]
  },
  {
   "source": [
    "Define general training function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(n_epochs, \n",
    "                 optimizer, model, \n",
    "                 loss_fn, metric_obj,\n",
    "                 training_data, \n",
    "                 n_examples,\n",
    "                 forward_only=False):\n",
    "    ''' Run full training loop \n",
    "    Args:\n",
    "        n_epochs (int): number of epochs to train\n",
    "        optimizer (keras.optimizers or str): optimizer\n",
    "        model (keras.Model): model ot train\n",
    "        loss_fn (keras.losses.Loss, function or str): loss function\n",
    "        training_data (TFDataset): dataset to train on \n",
    "        forward_only (bool): if True, does not update gradients\n",
    "    '''\n",
    "    for epoch in range(n_epochs):\n",
    "        tf.print(f'Epoch {int(epoch)}')\n",
    "        losses_train, metrics_train = run_epoch(optimizer, model, \n",
    "                                                loss_fn, metric_obj,\n",
    "                                                training_data, \n",
    "                                                n_examples,\n",
    "                                                forward_only)\n",
    "        metric_obj.reset_states()\n",
    "    return losses_train, metrics_train"
   ]
  },
  {
   "source": [
    "Train FNN "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      " 1/10 [==>...........................] - ETA: 40s - loss: 0.3590 - correct: 1.0000The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      " 2/10 [=====>........................] - ETA: 35s - loss: 0.1741 - correct: 1.0000The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "10/10 [==============================] - 50s 5s/step - loss: 0.2006 - correct: 1.0000\n"
     ]
    }
   ],
   "source": [
    "losses_nn1_nt, metric_nn1_nt = run_training(n_epochs=1, \n",
    "                                            optimizer=optimizer, \n",
    "                                            model=model1, \n",
    "                                            loss_fn=loss_model1, \n",
    "                                            metric_obj=metric,\n",
    "                                            training_data=ds_train.take(10),\n",
    "                                            n_examples=10,\n",
    "                                            forward_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_nn1, metric_nn1 = run_training(n_epochs=1, \n",
    "                                      optimizer=optimizer, \n",
    "                                      model=model1, \n",
    "                                      loss_fn=loss_model1, \n",
    "                                      metric_obj=metric,\n",
    "                                      training_data=ds_train.take(10),\n",
    "                                      n_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_nn2, metric_nn2 = run_training(n_epochs=1, \n",
    "                                      optimizer=optimizer, \n",
    "                                      model=model2, \n",
    "                                      loss_fn=loss_model2, \n",
    "                                      metric_obj=metric,\n",
    "                                      training_data=ds_train.take(10), \n",
    "                                      n_examples=10)\n",
    "# Fix:     \n",
    "# /Users/rr48396/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py:702 invalid_creator_scope\n",
    "# \"tf.function-decorated function tried to create \"\n",
    "# ValueError: tf.function-decorated function tried to create variables on non-first call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other:\n",
    "# Need to add baseline model\n",
    "# Add validation"
   ]
  }
 ]
}