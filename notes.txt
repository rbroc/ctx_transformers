# UP NEXT
- Names for various prep steps (and scripts?)
- Rename utils and refactor utils
    - Move to file when needed?
    - Is tokenization a script rather than util?

SET UP TOKENIZATION
- separate file
- separate folder for out

MAKE DATASET
- Stack and tf record

##### SPLIT UTILS
Split utils for:
- Dataset building
- TFRecord?
- Computations of sort?
- Preprocessing


#### RERUN MAKE_DATASET
    - Need new utils for make triplet loss dataset 
    - Add ID to example
    - Store mapping of ids somewhere
    - Reverse dataset at save
    - More years of data

# Edit tfrecord for new data format

# Quick try on notebook to make sure strategy works

LONGER TERM
- Check dependencies / setup.py
- Develop viz module
- Change package name
- Edit dockerfile
- Add gradient accumulation?
- Add to loss distance between n and anch
- Rerun baseline model with new scripts

Notes: 
- Margin, grad accumulation, model type as manipulations
- No room for optimizer in file naming protocol!
- Tolerance for difference in tests set to 5e-7
- In FFN anchor is averaged
- Average distance from all encodings removed