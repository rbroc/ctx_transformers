# UP NEXT

# To dos:
###### RERUN PREPROCESSING
# Drop duplicates early on
# Subset string?
# What to do with aggregation by subreddit? Remove?
# Make number of posts subreddits specifiable on command line
# Define meaningful paths

##### SPLIT UTILS
Split utils for:
- Dataset building
- TFRecord?
- Computations of sort?
- Preprocessing

#### RERUN TOKENIZATION
- seprate file
- separate folder

#### RERUN MAKE_DATASET
    - Need new utils for triplet loss dataset 
    - Add ID to example
    - Store mapping of ids somewhere
    - Reverse dataset at save
    - More years of data

# Edit tfrecord for new data format

# Quick try on notebook to make sure strategy works

LONGER TERM
- Check dependencies
- Develop viz module
- Change package name
- Edit dockerfile
- Add gradient accumulation?
- Add to loss distance between n and anch
- Rerun baseline model with new scripts

Notes: 
- Margin, grad accumulation, model type as manipulations
- No room for optimizer in file naming protocol!
- Tolerance for difference in tests set to 5e-7
- In FFN anchor is averaged
- Average distance from all encodings removed